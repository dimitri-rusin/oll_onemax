# Research question: Which plays a bigger role in causing PPO to learn a better policy: having only a few actions or having several types of actions?

# All and specific actions, using one type of actions
- action_type: [DISCRETE]
  dimensionality: [80]
  max_training_timesteps: [500_000]
  lambdas: [[1, 2, 3, 4, 5, 6, 7, 8], [5, 6, 7, 8], [2, 4, 6, 7, 8]]
  num_environments: [4]
  num_evaluation_episodes: [1_000]
  num_timesteps_per_evaluation: [4_000]
  ppo:
    batch_size: [100]
    clip_range: [0.2]
    ent_coef: [0.0]
    gae_lambda: [0.95]
    gamma: [1]
    learning_rate: [0.0003]
    n_epochs: [10]
    n_steps: [2_000]
    net_arch: [[50, 50]]
    policy: [MlpPolicy]
    vf_coef: [0.5]
  slurm:
    account: [sc122-dimitri]
    cpus_per_task: [1]
    nodes: [1]
    ntasks_per_node: [1]
    partition: [standard]
    qos: [standard]
    time: ["07:00:00"]
  closeness_to_optimum: [0.95, 0.8, 0.7]
  random_seed: "list(range(20))"
  reward_type: [EVALUATIONS_PLUS_FITNESS]
  state_type: [ONE_HOT_ENCODED]

# All actions, using several types of actions
- action_type: [DISCRETE]
  dimensionality: [80]
  max_training_timesteps: [500_000]

  # mutation_rates = mutation_sizes / dimensionality
  # crossover_rates = 1.0 / crossover_sizes

  mutation_rates: [[0.0125, 0.025, 0.0375, 0.05, 0.0625, 0.075, 0.0875, 0.1]]
  mutation_sizes: [[1, 2, 3, 4, 5, 6, 7, 8]]
  crossover_rates: [[1.0, 0.5, 0.33333333, 0.25, 0.2, 0.16666667, 0.14285714, 0.125]]
  crossover_sizes: [[1, 2, 3, 4, 5, 6, 7, 8]]
  num_environments: [4]
  num_evaluation_episodes: [1_000]
  num_timesteps_per_evaluation: [4_000]
  ppo:
    batch_size: [100]
    clip_range: [0.2]
    ent_coef: [0.0]
    gae_lambda: [0.95]
    gamma: [1]
    learning_rate: [0.0003]
    n_epochs: [10]
    n_steps: [2_000]
    net_arch: [[50, 50]]
    policy: [MlpPolicy]
    vf_coef: [0.5]
  slurm:
    account: [sc122-dimitri]
    cpus_per_task: [1]
    nodes: [1]
    ntasks_per_node: [1]
    partition: [standard]
    qos: [standard]
    time: ["07:00:00"]
  closeness_to_optimum: [0.95, 0.8, 0.7]
  random_seed: "list(range(20))"
  reward_type: [EVALUATIONS_PLUS_FITNESS]
  state_type: [ONE_HOT_ENCODED]

# Specific actions, using several types of actions
- action_type: [DISCRETE]
  dimensionality: [80]
  max_training_timesteps: [500_000]
  mutation_rates: [[0.0625, 0.075, 0.0875, 0.1], [0.025, 0.05, 0.075, 0.0875, 0.1]]
  mutation_sizes: [[5, 6, 7, 8], [2, 4, 6, 7, 8]]
  crossover_rates: [[0.2, 0.16666666666666666, 0.14285714285714285, 0.125], [0.5, 0.25, 0.16666666666666666, 0.14285714285714285, 0.125]]
  crossover_sizes: [[5, 6, 7, 8], [2, 4, 6, 7, 8]]
  num_environments: [4]
  num_evaluation_episodes: [1_000]
  num_timesteps_per_evaluation: [4_000]
  ppo:
    batch_size: [100]
    clip_range: [0.2]
    ent_coef: [0.0]
    gae_lambda: [0.95]
    gamma: [1]
    learning_rate: [0.0003]
    n_epochs: [10]
    n_steps: [2_000]
    net_arch: [[50, 50]]
    policy: [MlpPolicy]
    vf_coef: [0.5]
  slurm:
    account: [sc122-dimitri]
    cpus_per_task: [1]
    nodes: [1]
    ntasks_per_node: [1]
    partition: [standard]
    qos: [standard]
    time: ["07:00:00"]
  closeness_to_optimum: [0.95, 0.8, 0.7]
  random_seed: "list(range(20))"
  reward_type: [EVALUATIONS_PLUS_FITNESS]
  state_type: [ONE_HOT_ENCODED]
