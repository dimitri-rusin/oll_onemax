


This should be the step function of the environment:
```py
# mutation phase
p = lambda_ / self.num_dimensions
xprime, f_xprime, ne1 = x.mutate(p, lambda_, rng)

# crossover phase
c = 1 / lambda_
y, f_y, ne2 = x.crossover(xprime, c, lambda_, include_xprime_crossover, count_different_inds_only,  rng)

# selection phase
old_f_x = f_x
if f_x <= f_y:
  x = y
  f_x = f_y
```










NGUYEN INFO
==============================================================================

I have an example script for doing the plotting (the learning curve plot and the policy plot) for the LeadingOnes benchmark here: https://github.com/ndangtt/LeadingOnesDACNew/blob/main/examples/analyse_results.py

You can try that script by following example 2:

https://github.com/ndangtt/LeadingOnesDACNew/tree/main

Here is our FOGA paper published year on using irace to control the parameter of the (1+lambda,lambda)-GA on OneMax problem:

https://arxiv.org/abs/2302.12334

Here is the algorithm in Python:

https://github.com/DE0CH/OLL/blob/ceeb3b118291cc72bfe3a40c1577983bf487ac41/tuned-with-irace/onell_algs.py#L445

(the lbds parameter is the policy, it's an array of size n, where the i^th element tells us which lambda value we should use for fitness value of i)

Deyao (my student) also reimplemented this algorithm in Rust. It significantly reduces the compute time, we can use this one for the evaluation (while using the Python code for the training):

https://github.com/DE0CH/OLL/tree/ceeb3b118291cc72bfe3a40c1577983bf487ac41/onell_algs_rs


It was very nice to meet you yesterday. Here is the SAT solving paper I mentioned yesterday:

https://arxiv.org/abs/2211.12581

Another thing during our conversion was the topic that Carola, Martin, and our collaborators in Freiburg and I are working on at the moment is Dynamic Algorithm Configuration (DAC). We focuses on developing new DAC benchmarks and using them to gain understanding the strengths and weakness of current deep-RL methods in the context of DAC. I thought I'd share our GECCO 2022 here with you as well, just in case you might be interested:

Paper: https://arxiv.org/abs/2202.03259
Blog post: https://andrebiedenkapp.github.io/blog/2022/gecco/
GECCO presentation video: https://www.youtube.com/watch?v=xgljDu5qE-w



February 6, 2024
==============================================================================

So, I just generated a bunch of .zip files. I just want one policy. Can I get that right? Just one policy. Out of this whole thing. Just one policy, essentially save that one policy. Load it, and evaluate for every fitness value, please. Is it that hard?

I still want to generate a policy. Using the PPO project. But now I am handling the outputs. Just, the environment should work properly. Of course. So, I want it to work properly. Just be deterministin on the same input of the random seed. Hmm, let me check that. What are the outputs of the two result functions?

February 7, 2024
==============================================================================

We still could not visualize a single policy in this bitch. We just want a policy: gets a fitness, spits out a lambda. That's all. This should come OUT of using stable_baselines3.PPO. This is ALL that I am asking for.

This function `make_vec_env` actually calls reset with some uncontrolled number as the random seed. This messes up reproducability. So, we work without this function for now.

One could use an actual SQLite database here to save the fitnesses and corresponding lambdas. But anyway, one just needs the different policies. They should be rewritten down. Almost no real need to visualize. Except maybe with a bar chart. Why not, even 100 fitnesses, would be fine. Which is equivalent with having 100 dimensions. Then, all we really need to add is just an actual improvement of the policy. Over time, for each policy, we should probably evaluate it. The "Tensorboard" software could be used, but probably not. But, in any case, it's better to have all the data, in a static way, in a real way, right here on disk. So, the main goal in the end, is of course, to use the real OneMax plus OLL environment and algorithm and at the same time achieve a sequence of constantly improving policies over time, just over time, so basically just a sequence of these. We want to get them. To have them written down. To have them written down right here, right now. Maybe, this won't be too hard. You know, just write it down. Just write it. And then, this is it. Actually. Just this: real environment. Plus ever improving policies.

Just to re-create the env.

