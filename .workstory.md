






Get the mesu prepared stuff to my local system for visualization.
```sh
scp mesu:/scratchbeta/rusind/oll_onemax/data/policies.db .
```
















```sh
#!/usr/bin/env bash

#PBS -q beta
#PBS -l select=1:ncpus=24
#PBS -l walltime=00:01:00
#PBS -N onemaxoll
#PBS -j oe

module add cmake/3.22
module add conda3-2023.02
module add gcc/11.2
module add git/2.42.0
module add LLVM/clang-llvm-10.0

conda activate /scratchbeta/rusind/oll_onemax/.conda_environment/
python /scratchbeta/rusind/oll_onemax/strategies.py
```




















```sh
ssh mesu

module add cmake/3.22
module add conda3-2023.02
module add gcc/11.2
module add git/2.42.0
module add LLVM/clang-llvm-10.0



cd /scratchbeta/rusind/
rm -rf oll_onemax/
git clone --branch tabular --depth 1 git@github.com:dimitri-rusin/oll_onemax.git
cd oll_onemax
git submodule init
git submodule update

conda activate base
rm -rf ./.conda_environment/
conda env create --prefix ./.conda_environment/ --file .conda.yaml
conda activate ./.conda_environment/
pip install --requirement .pip.txt


```













```sh
ssh mesu

cd francois/
./RUN

ll /scratchbeta/rusind/
cat /scratchbeta/rusind/4c_20_LogFile.txt
cat /scratchbeta/rusind/4c_20.jl

qsub RUN

qstat 2006960.mesu2
qstat 2006960.mesu2 -x
qdel 2006960.mesu2

rm -rf /scratchbeta/rusind/4c_19*

scp mesu:/scratchbeta/rusind/4c_20_LogFile.txt .
scp mesu:/scratchbeta/rusind/4c_20.jl .
```


















Install a Rust-implemented python module
```sh
pip install maturin
maturin build --release
pip install target/wheels/onell_algs_rs-0.1.0-cp310-cp310-manylinux_2_31_x86_64.whl

import onell_algs_rs
result = onell_algs_rs.onell_lambda(n, lbds, seed, max_evals)
```




This should be the step function of the environment:
```py
# mutation phase
p = lambda_ / self.num_dimensions
xprime, f_xprime, ne1 = x.mutate(p, lambda_, rng)

# crossover phase
c = 1 / lambda_
y, f_y, ne2 = x.crossover(xprime, c, lambda_, include_xprime_crossover, count_different_inds_only,  rng)

# selection phase
old_f_x = f_x
if f_x <= f_y:
  x = y
  f_x = f_y
```










NGUYEN INFO
==============================================================================

I have an example script for doing the plotting (the learning curve plot and the policy plot) for the LeadingOnes benchmark here: https://github.com/ndangtt/LeadingOnesDACNew/blob/main/examples/analyse_results.py

You can try that script by following example 2:

https://github.com/ndangtt/LeadingOnesDACNew/tree/main

Here is our FOGA paper published year on using irace to control the parameter of the (1+lambda,lambda)-GA on OneMax problem:

https://arxiv.org/abs/2302.12334

Here is the algorithm in Python:

https://github.com/DE0CH/OLL/blob/ceeb3b118291cc72bfe3a40c1577983bf487ac41/tuned-with-irace/onell_algs.py#L445

(the lbds parameter is the policy, it's an array of size n, where the i^th element tells us which lambda value we should use for fitness value of i)

Deyao (my student) also reimplemented this algorithm in Rust. It significantly reduces the compute time, we can use this one for the evaluation (while using the Python code for the training):

https://github.com/DE0CH/OLL/tree/ceeb3b118291cc72bfe3a40c1577983bf487ac41/onell_algs_rs


It was very nice to meet you yesterday. Here is the SAT solving paper I mentioned yesterday:

https://arxiv.org/abs/2211.12581

Another thing during our conversion was the topic that Carola, Martin, and our collaborators in Freiburg and I are working on at the moment is Dynamic Algorithm Configuration (DAC). We focuses on developing new DAC benchmarks and using them to gain understanding the strengths and weakness of current deep-RL methods in the context of DAC. I thought I'd share our GECCO 2022 here with you as well, just in case you might be interested:

Paper: https://arxiv.org/abs/2202.03259
Blog post: https://andrebiedenkapp.github.io/blog/2022/gecco/
GECCO presentation video: https://www.youtube.com/watch?v=xgljDu5qE-w



February 6, 2024
==============================================================================

So, I just generated a bunch of .zip files. I just want one policy. Can I get that right? Just one policy. Out of this whole thing. Just one policy, essentially save that one policy. Load it, and evaluate for every fitness value, please. Is it that hard?

I still want to generate a policy. Using the PPO project. But now I am handling the outputs. Just, the environment should work properly. Of course. So, I want it to work properly. Just be deterministin on the same input of the random seed. Hmm, let me check that. What are the outputs of the two result functions?

February 7, 2024
==============================================================================

We still could not visualize a single policy in this bitch. We just want a policy: gets a fitness, spits out a lambda. That's all. This should come OUT of using stable_baselines3.PPO. This is ALL that I am asking for.

This function `make_vec_env` actually calls reset with some uncontrolled number as the random seed. This messes up reproducability. So, we work without this function for now.

One could use an actual SQLite database here to save the fitnesses and corresponding lambdas. But anyway, one just needs the different policies. They should be rewritten down. Almost no real need to visualize. Except maybe with a bar chart. Why not, even 100 fitnesses, would be fine. Which is equivalent with having 100 dimensions. Then, all we really need to add is just an actual improvement of the policy. Over time, for each policy, we should probably evaluate it. The "Tensorboard" software could be used, but probably not. But, in any case, it's better to have all the data, in a static way, in a real way, right here on disk. So, the main goal in the end, is of course, to use the real OneMax plus OLL environment and algorithm and at the same time achieve a sequence of constantly improving policies over time, just over time, so basically just a sequence of these. We want to get them. To have them written down. To have them written down right here, right now. Maybe, this won't be too hard. You know, just write it down. Just write it. And then, this is it. Actually. Just this: real environment. Plus ever improving policies.

Just to re-create the env.

Was machen wir mit Nguyen? Wir wollen ein Tensorboard mit den Policies, ganz klar. Ganz klar. Für jeden Step, DEN wir mappen, wollen wir die Policy. Die exakte. Von dem Timestep, von dem Zeitpunkt, und keinem anderen. Das war's eigentlich. Und dann wollen wir die Learning Curve. Wie kriegen wir die Kurve hin? Die Kurve.

Die Kurve ist einfach nur die Evaluation. Die Evaluation der tatsächlichen Policy zu dem konkreten Zeitpunkt. Es gibt mehrere Zeitpunkte: Funktions-Evaluationen, Training minibatch consumptions, timesteps, episodes. Man könnte beliebiges davon als Zeitpunkt nehmen. Oder sogar: Sekunde. Wallclock. Oder auch CPU-time. Ja, im Endeffekt, das sind vielleicht sogar fast alle Möglichkeiten. Also, dann, zu diesem Zeitpunkt: die Policy plus Evaluation. Die Policy-Evaluation geht über eine feste Anzahl von Episoden, bis zu dem Moment, wo wir den vollständigen Return bekommen. Dann average. Das ist die Policy-Evaluation.

Nguyen's job: all we really want is some policies.

Nguyen's job: Let's evaluate the policies. Die Policies müssen alle geradlinig sein. Jede Policy muss rein vor einem erscheinen, so wie die Policy wirklich ist. In Wahrheit. Gott, offenbare mir die Policy. Natürlich das aller letzte Ziel überhaupt ist ein Journal-Paper. Eine Erweiterung, ansetzend, angreifend. Wir wollen weiter veröffentlichen. In das Loch. In das Ziel. Herein. Hinein-kippend. Das Paper... wird nicht nur diese Technik beinhalten. Nein, nein. Wir wollen PPO ausprobieren, aber nur weil Nguyen das so sagt. Die eigentliche Wahrheit ist: wir wollen neural MCTS ausprobieren. Wird das klappen? Wir wollen NRPA ausprobieren. Wird das klappen? Eine Policy wird developed. Eine Policy, welche nicht und überhaupt nicht vom Zustand abhängt, ganz genau so wie RLS_1 und RLS_5. Bitflip, crossover. Alles das - ist möglich - mit NRPA. Mit NRPA Policy Adaptation. Very simply so. Has it been tried? Who tried it so. Then? What came out of it?

Das ist nur die Sekunde, dies ist nur der Moment. Wann wird es festgelegt? Aber später diese andere. Ah.

With NRPA, we try MCTS, too. It's very simple. But it has to be generated. Generated, and properly so.

But for now, PPO will be tried. You just have to plug it in at the correct position. Log the policies. Evaluate them. That's enough. Make a config file. Keep it simple. For now. Thank you.

